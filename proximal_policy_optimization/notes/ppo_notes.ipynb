{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.TRPO代理目标函数的解释\n",
    "## 1.1 数学符号的介绍\n",
    "### 1.1.1 数学期望 $\\mathbb{E}$\n",
    "(1)概念：一个随机变量的期望（或者期望值）指的是其概率分布集中趋势的一种度量。\n",
    "\n",
    "(2)数学原理：\n",
    "![](./images/数学期望的数学原理.png)\n",
    "\n",
    "(3)期望的性质：\n",
    "线性、非负性（如果随机变量非负）、独立性（如果随机变量是互相独立的）。注意后两个性质是有前提条件的。\n",
    "![](./images/数学期望的性质.png)\n",
    "\n",
    "(4)数学期望在RL中的应用：\n",
    "主要用于评估plicy的性能。\n",
    "![](./images/数学期望在RL中的应用.png)\n",
    "\n",
    "### 1.1.2 优势函数 Advantage Function\n",
    "（1）概念：\n",
    "通过量化在特定状态下采取某个行动的相对质量，与在该状态下所有可能行动的平均预期质量之间的差距，来帮助提高策略优化过程中的稳定性和效率。\n",
    "\n",
    "（2）数学原理\n",
    "![](./images/优势函数的数学原理.png)\n",
    "\n",
    "（3）优势函数的性质\n",
    "相对比较、泛华性、信号可解释性。\n",
    "![](./images/数学期望的性质.png)\n",
    "\n",
    "（4）优势函数的RL应用\n",
    "![](./images/优势函数的应用_1.png)\n",
    "![](./images/优势函数的应用_2.png)\n",
    "![](./images/优势函数的应用_3.png)\n",
    "\n",
    "### 1.1.3 KL散度 Kullback-Leibler(KL) divergence\n",
    "（1）概念：KL散度主要用于量化两个概率分布之间的差异。\n",
    "\n",
    "（2）数学原理：\n",
    "分为离散分布和连续分布两种情况。\n",
    "![](./images/KL散度的数学原理.png)\n",
    "\n",
    "特殊情况：\n",
    "![](./images/KL散度的特殊计算情况.png)\n",
    "\n",
    "（3）KL散度的性质\n",
    "非负性、不对称性、与熵之间的关系、凸性。\n",
    "![](./images/KL散度的性质.png)\n",
    "\n",
    "（4）KL散度的应用例子\n",
    "![](./images/KL散度的应用例子_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.TRPO目标函数的求解过程\n",
    "![](./images/TRPO求解的过程概览.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 定义策略优化的目标：\n",
    "策略优化的目标，就是最大化期望的期望函数的值。\n",
    "![](./images/TRPO目标函数求解步骤_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 定义代理目标：\n",
    "![](./images/TRPO目标函数求解步骤_2.jpg)\n",
    "\n",
    "这里之所以叫做代理目标，是因为TRPO并不按照1中的方式，直接优化目标函数，而是对一个新的代理目标进行优化，具体为什么优化代理目标就能得到优化原始目标同样的策略优化效果，具体的数学证明如下。\n",
    "![](./images/TRPO代理目标函数与原目标函数的等价证明.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 引入置信域约束：\n",
    "引入置信域约束的主要原因是在代理目标函数的直接使用朴素梯度下降会导致策略的大幅度更新，导致泛化性差并且难以收敛。而TRPO使用KL散度解决了这个问题，限制了在一次更新中，策略更新的幅度，防止了灾难性的大幅度策略更新。\n",
    "![](./images/TRPO目标函数求解步骤_3.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 构造约束的优化问题：\n",
    "![](./images/TRPO目标函数求解步骤_4.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 使用自然梯度下降求解：\n",
    "由于KL散度的约束导致无法直接求解，因此使用二次估计对KL散度进行近似。近似完成之后就可以使用自然梯度下降法来进行求解。\n",
    "![](./images/TRPO目标函数求解步骤_5.jpg)\n",
    "\n",
    "### 2.5.1 二次估计 quadratic approximation\n",
    "在TRPO中的主要作用就是使用二阶泰勒展开对KL散度约束项进行估计。\n",
    "![](./images/二次估计KL散度的数学过程.jpg)\n",
    "\n",
    "### 2.5.2 Fisher information matrix(FIM)\n",
    "FIM的主要作用是作为KL散度的Hessian(二阶导)近似，使得自然梯度能够高效计算。\n",
    "![](.//images/FisherInformationMatrix.jpg)\n",
    "\n",
    "### 2.5.3 Natural Gradient Descent\n",
    "NGD在TRPO中的主要作用就是求解目标函数的。\n",
    "![](./images/自然梯度下降法.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 使用共轭梯度进行更新\n",
    "使用共轭梯度算法原因主要是因为计算自然梯度的方向时，计算开销太大，这里的开销主要指的是计算矩阵的逆的时候，开销很大。\n",
    "![](./images/TRPO目标函数求解步骤_6.jpg)\n",
    "\n",
    "### 2.6.1 共轭梯度算法Cojugate gradient algorithm\n",
    "![](./images/共轭梯度算法原理.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 使用线搜索来进行最终的更新\n",
    "使用线搜索的主要原因是为找到最佳的step size $\\alpha$ <br>\n",
    "![](./images/TRPO目标函数求解步骤_7.jpg)\n",
    "\n",
    "### 2.7.1 Line Search\n",
    "线搜索是一种优化方法，通常用于在给定方向下移动时找到最佳的step size。通常与基于梯度的算法结合，比如梯度下降，共轭梯度等。\n",
    "![](./images/线搜索的数学原理.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.裁剪代理目标Clipped Surrogate Objective\n",
    "## 3.1 动机\n",
    "TRPO的代理目标是$L^{CPI}$，CPI指的是传统的策略迭代，存在的问题是，如果在没有KL散度的约束条件下，直接优化CPI会导致policy的过度更新。因此PPO修改了目标，该目标中惩罚了概率比的远离1的时候的更新（意思就是policy更新的幅度）。\n",
    "\n",
    "## 3.2 修改后的目标\n",
    "![](./images/clipped代理目标.jpg)\n",
    "\n",
    "1> 其中第一项其实就是原始的CPI的目标，第二项限制了概率比在一个由$\\epsilon$限制的范围内，一般取值是0.2。<br>\n",
    "2> 第二项对概率比的裁剪，会是的概率比的移动会被限制在这个区间内。<br>\n",
    "3> $L^{CLIP}$实际上是$L^{CPI}$的一个下界。通过这种方式，会忽略使得policy变得更好的概率比变化，而会纳入使得policy变差的变化<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  裁剪代理目标的性质\n",
    "（1）概率比是在$1-\\epsilon$被裁剪还是$1 + \\epsilon$，是取决于优势函数的值A是大于0还是小于0的。<br>\n",
    "A > 0的时候，在$1 + \\epsilon$被裁剪；<br>\n",
    "A < 0的时候，在$1-\\epsilon$被裁剪。<br>\n",
    "![](./images/clipped代理目标的性质_1.jpg)<br>\n",
    "\n",
    "（2）$L^{CLIP}$实际上是$L^{CPI}$的一个下界,当policy更新幅度太大的时候，会被惩罚。\n",
    "![](./images/clipped代理目标的性质_2.jpg)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.自适应KL惩罚系数Adaptive KL Penalty Coefficient\n",
    "## 4.1 动机\n",
    "为了提出一个clipped surrogate objective的替代选择，或者是与之结合使用的选择：那就是在KL散度上添加惩罚系数。\n",
    "\n",
    "## 4.2 实际的效果：\n",
    "这种在KL散度上添加惩罚系数的效果比裁剪代理目标的效果差，但是在这儿作为baseline提出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 算法的步骤\n",
    "(1)使用随机梯度下降算法优化$L^{KLPEN}$；<br>\n",
    "(2) 计算KL散度的期望 d，并且根据 d 的大小来更新惩罚系数 $\\beta$。<br>\n",
    "(3)算法流程中参数更新的解释：更新d的判断条件中的常数是超参数，但是算法对他们的值并不敏感。而且惩罚系数 $\\beta$的初始值也不重要，因为过程中它很快就会被调整。\n",
    "![](./images/自适应KL惩罚系数.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.算法\n",
    "## 5.1 最终的目标函数\n",
    "目标函数组成部分的介绍：<br>\n",
    "（1）第一项就是之前介绍的裁剪代理目标；<br>\n",
    "（2）第二项是价值函数的loss（也叫做critic loss），是均方误差，用于降低梯度更新过程中的variance.<br>\n",
    "（3）第三项是熵增（Entropy Loss），主要的作用是防止policy陷入局部最优。<br>\n",
    "![](./images/PPO最终的目标函数.jpg)<br>\n",
    "![](./images/PPO最终的目标函数_价值函数loss.jpg)<br>\n",
    "![](./images/PPO最终的目标函数_熵loss.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 补充知识：variance 和 bias的区别与联系\n",
    "![](./images/偏差和方差的联系.jpg)\n",
    "![](./images/偏差和方差的区别.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 梯度更新的实现\n",
    "### 5.2.1 PPO的算法流程\n",
    "在一次迭代过程中：<br>\n",
    "(1)对N个并行的actors，分别收集T个时间步的结果；<br>\n",
    "(2)在NT个时间步的数据上，构建 surrogate loss，并且使用SGD或者Adam优化器进行优化K个epochs。<br>\n",
    "![](./images/PPO算法流程.jpg)<br>\n",
    "\n",
    "### 5.2.2 广义优势估计\n",
    "在强化学习中，策略梯度方法需要一种方式来估计优势函数（advantage function）和回报（Return），以指导策略更新和价值函数优化。PPO就是使用了GAE来对优势函数进行平滑计算，兼顾短期和长期的奖励信号。具体来讲，就是在构建surrogate loss的过程中，会需要得到$L^{CLIP}$中优势函数的advantage estimator。在PPO中是使用truncated version of generalized advantage estimation广义优势估计（GAE）得到的。具体的数学表达式如下：<br>\n",
    "这里公式11的最后一项的上标错了，正确的应该是T-t-1。可以参考https://www.zhihu.com/question/5395314304。\n",
    "![](./images/公式11的推导.jpg)<br>\n",
    "![](./images/PPO_GAE.jpg)<br>\n",
    "\n",
    "![](./images/PPO_GAE_数学原理.jpg)<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
