{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "[labml](https://nn.labml.ai/rl/ppo/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClippedPPOLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, \n",
    "                log_pi: torch.Tensor,\n",
    "                sampled_log_pi: torch.Tensor,\n",
    "                advantage: torch.Tensor, \n",
    "                clip: float #这个就是epsilon\n",
    "                ) -> torch.Tensor: \n",
    "        \n",
    "        # 计算概率比\n",
    "        ratio  = torch.exp(log_pi - sampled_log_pi)\n",
    "        \n",
    "        # 剪切概率比，限制policy更新的幅度，会将ratio中的值都限制到[1-clip, 1+clip]\n",
    "        clipped_ratio = ratio.clamp(min = 1.0 - clip, max = 1.0 + clip)\n",
    "\n",
    "        # 计算策略的奖励\n",
    "        policy_reward = torch.min(ratio * advantage, clipped_ratio * advantage)\n",
    "\n",
    "        # 计算 最终的ppo loss：求期望，并且加负号\n",
    "        clipped_ppo_loss = -policy_reward.mean()\n",
    "\n",
    "        return clipped_ppo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.clamp()函数的介绍\n",
    "![](./images/clamp函数的介绍.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
